{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c4b0def",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a601253",
   "metadata": {},
   "source": [
    "# What we know:\n",
    "\n",
    "#### The kaggle provided dataset contains 3 files:  \n",
    "##### comment_to_score.csv\n",
    "    - Contains all unranked comments. Our objective is to rank these comments based off the toxicity, with 1 being the most toxic. \n",
    "\n",
    "##### sample_submission.csv\n",
    "    - Contains an example of how the submission should look like... Not sure if this will help with training?\n",
    "\n",
    "##### validation_data.csv\n",
    "    - According to kaggle this is the \"pair rankings that can be used to validate models. Includes annotator work id and how the annotator ranked a given pair of comments\"\n",
    "    \n",
    "# Understanding the objective\n",
    "Before moving on, let's take a moment and understand what our overall objective is: \n",
    "We want to rank a list of comments base off of the level of toxicity without the use of any provided training data. However, while we do not have training data, we have provided validation data, which includes a set of toxicity rankings that might help to validate models.\n",
    "\n",
    "# Understanding our data:\n",
    "In the next few lines, we will import our data and see what we're actually dealing with and see if the data provided is actually useful. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8142faa9",
   "metadata": {},
   "source": [
    "#### comments_to_score.csv\n",
    "        - We knew how the data looked like before, but let's take a closer look and see what might need to be cleaned up  \n",
    "   \n",
    "   - Column Names\n",
    "       - comment_id (comment identifier)\n",
    "       - text (actual text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "adf8fe35",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"\n",
      " \n",
      "\n",
      "Gjalexei, you asked about whether there is an \"\"anti-editorializing\"\" policy here.  There is, and it's called wikipedia:neutral point of view.  It discusses at some length  the case of what we should do when writing about a subject which most of us find repugnant.  Whilst you're not likely to get too many defenders of FGM here, the need for the policy should be clearer for articles like abortion, for instance.\n",
      "\n",
      "If something you write is edited and you're not sure why, please continue to question such edits on the talk page.  Sometimes, you'll learn more about wikipedia policy.  Sometimes, you'll find out that some other people working on here can get it flat-out wrong ) Robert Merkel\"\n"
     ]
    }
   ],
   "source": [
    "# comments_to_score data\n",
    "df = pd.read_csv(\"./data/comments_to_score.csv\")\n",
    "eg1 = df.text[0]\n",
    "print(eg1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1b21869",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"\n",
      "\n",
      "I already gave you the source for all my edits   The Slur database, so I will not continue to play your little game. If you weren't so lazy and intent on harassment, you could use Google to search for \"\"Gargamel\"\" and \"\"Jew\"\". You get more than 400 hits including white supremacist sites and an academic paper dating to 1996. It's obviously a real slur with some usage. Nice try at being obdurate though. I'm sure there's a slur that describes that characteristic. Shyster doesn't quite cover it.  22:08, 22 Dec 2004 (UTC)\"\n"
     ]
    }
   ],
   "source": [
    "eg2 = df.text[25]\n",
    "print(eg2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14235b9b",
   "metadata": {},
   "source": [
    "##### took two comments from the provided dataset. We can make a few observations based off of the content of the texts:   \n",
    "  \n",
    "- First, punctuation. Normally, when we deal with datasets that involve texts, we would ignore punctuation. Howevever, knowing the habits of the internet, the punctuation can reflect toxicity. For instance, quotation marks have been used to represent sarcasm.\n",
    "    -Building onto this, this same concept applies with capitalized letters. Generally speaking, when dealing with texts, we would try to make the letters consistent. However, given the habits of the internet, we can \n",
    "    \n",
    "- I believe we can consider the second comment to be more toxic. But let's try to understand what makes it toxic: \n",
    "    - \"If you weren't so lazy and intent on harassment, you could use Google to search for \"\"Gargamel\"\" and \"\"Jew\"\"\". \n",
    "        - This particular sentence feels targeted, with lazy being the negative description.   \n",
    "        - Might want to consider pulling common words, phrases, and general patterns with the provided validation dataset as a means to start. However, keep in mind that the dataset also contains data that is not in our testing set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3770d263",
   "metadata": {},
   "source": [
    "###### Validation_data.csv\n",
    "After playing around with the data, I've come to realize the order of the comments do not matter. As described on Kaggle, this ia a collection of comments, which a person judged a pair of comments to see which one is more or less toxic (making almost binary). The comments are kinda hilarious from a third person perspective.  \n",
    "  \n",
    "Looking at the data on excel, I noticed that some comments are repeated, showing that it's been consistently rated as being more toxic than the other. It might be interesting to make a histogram based off of the number times a comment is rated as toxic. We can also create a list of toxic comments and have it ordered from most toxic to least toxic based off the number of counts it was rated as more toxic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1a9047a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "worker                                                      313\n",
       "less_toxic              This article sucks \\n\\nwoo woo wooooooo\n",
       "more_toxic    WHAT!!!!!!!!?!?!!?!?!!?!?!?!?!!!!!!!!!!!!!!!!!...\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vd = pd.read_csv(\"./data/validation_data.csv\")\n",
    "eg3 = vd.loc[0,:]\n",
    "eg3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27c40f96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "worker                                                       46\n",
       "less_toxic                           I'm Jim,a retarded idiot .\n",
       "more_toxic     You are a nazi. \\n\\nYour defense of the Latin...\n",
       "Name: 100, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eg4 = vd.loc[100,:]\n",
    "eg4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e9dd66",
   "metadata": {},
   "source": [
    "##### sample_submission.csv\n",
    "This simply tells us how we should make our submission, it is interesting that we only care about the comment_id and the score. Although I am more interested in how the score will be developed. Is it unique or what?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ecbc9f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_id</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>114890</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>732895</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1139051</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1434512</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2084821</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2452675</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   comment_id  score\n",
       "0      114890    0.5\n",
       "1      732895    0.5\n",
       "2     1139051    0.5\n",
       "3     1434512    0.5\n",
       "4     2084821    0.5\n",
       "5     2452675    0.5"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eg5 = pd.read_csv(\"./data/sample_submission.csv\")\n",
    "eg5.loc[0:5,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7845a77",
   "metadata": {},
   "source": [
    "### Testing out Ideas\n",
    "\n",
    "In the previous section, I discussed ideas to explore. In this section, we will try out those ideas and see how they could apply to our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0c6e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Find all the unique comments in the validation dataset. \n",
    "\n",
    "# Alright, so I want to look at this data with ease. To save time and memory, I'm going to give each unique comment a unique number\n",
    "# And count the number of occurences the unique ID has. \n",
    "toxic_unique = vd.more_toxic.unique()\n",
    "toxic_dict = {};\n",
    "not_toxic_unique = vd.less_toxic.unique()\n",
    "not_toxic_dict = {};\n",
    "count = 0;\n",
    "\n",
    "for tword in toxic_unique:\n",
    "    toxic_dict[count] = [tword, len(vd.index[vd.more_toxic == tword].tolist())]\n",
    "    count += 1\n",
    "    # Create a dictionary that we can use\n",
    "    #I don't know how much time or memory this uses, but basicaly I am creating a dictionary which stores the number of counts per unique comment\n",
    "\n",
    "count = 0\n",
    "for ntword in not_toxic_unique:\n",
    "    not_toxic_dict[count] = [ntword, len(vd.index[vd.less_toxic == ntword].tolist())] # Create a dictionary that we can use\n",
    "    count +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f8aca0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "toxic_df = pd.DataFrame.from_dict(toxic_dict, orient = 'index', columns = ['text', 'count']).sort_values(by =['count'], ascending = False)\n",
    "not_toxic_df = pd.DataFrame.from_dict(not_toxic_dict, orient = 'index', columns = ['text', 'count']).sort_values(by = ['count'], ascending = False)\n",
    "\n",
    "toxic_df[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e7178b",
   "metadata": {},
   "outputs": [],
   "source": [
    "not_toxic_df[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62b40fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"toxic_df length: \", len(toxic_df))\n",
    "print(\"not_toxic_df length: \", len(not_toxic_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57456e12",
   "metadata": {},
   "source": [
    "As you can see, I created two different dataframes to rank the frequency of comments found in the toxic and less toxic columns from the validity data provided. One thing to keep in mind is that we will want our algorithm to generalize, therefore having a keyword classifer might not be particularly useful. Instead of looking for specific words, we should think about the underlying trends in the data.      \n",
    "  \n",
    "Interestingly, in the toxic column, a lot of the comments use an abundance of capital letters. Furthermore, comments deemed more toxic seem to have more misspelling, slang, and punctuation. Of course, this type of identification is for the most obvious cases. However, this leads me to wonder if we can apply an algorithm to identify underlying trends.   \n",
    "  \n",
    "I almost glossed over one thing: The goal of this project is to rank the comments from most toxic to least toxic. As explained in the project description, it is very difficult to rank a list of comments entirely. Instead, it is a lot easier to judge two comments and rank which one is toxic. I should keep this in mind when I'm building my future algorithm. Instead of loading the entire dataset into my model, it might be better to use a sliding window method that takes in two comments and spits out which one is considered more toxic. This leads me to think that a binary classifier might be particularly useful. However, how would we identify the unlabeled features? Hmmm.... How much natural language processing is actually necessary here?\n",
    "  \n",
    "On a side note, I think it might be interesting to see the amount of overlap between the two classes (more toxic vs less toxic)\n",
    "I find it funny that there's more toxic comments than not toxic comments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f745e5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_df = {}\n",
    "un_not_toxic_df = {}\n",
    "\n",
    "\n",
    "count = 0\n",
    "ncount = 0\n",
    "\n",
    "for i in range(len(not_toxic_df.text)):\n",
    "    ntword = not_toxic_df.text[i]\n",
    "    ntcount = not_toxic_df['count'][i]\n",
    "\n",
    "    if ntword in list(toxic_df.text):\n",
    "        t_idx = (toxic_df.index[toxic_df.text == ntword].tolist())[0]\n",
    "        t_count = toxic_df['count'][t_idx]\n",
    "        tword = toxic_df['text'][t_idx]\n",
    "        common_df[count] = [ntword, ntcount, t_count]\n",
    "        count += 1\n",
    "        \n",
    "    else:\n",
    "        un_not_toxic_df[ncount] = [ntword, ntcount]\n",
    "        ncount += 1\n",
    "        \n",
    "common_df = pd.DataFrame.from_dict(common_df, orient = 'index', columns = ['text', 'ntcount', 'tcount'])\n",
    "un_not_toxic_df = pd.DataFrame.from_dict(un_not_toxic_df, orient = 'index', columns = ['text', 'count']).sort_values(by = ['count'], ascending = False)\n",
    "un_not_toxic_df = un_not_toxic_df.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc50466",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "un_toxic_df = {}\n",
    "\n",
    "for i in range(len(toxic_df)):\n",
    "    tword = toxic_df.text[i]\n",
    "    if tword not in list(common_df.text):\n",
    "        tcount = toxic_df['count'][i]\n",
    "        un_toxic_df[count] = [tword, tcount]\n",
    "        count += 1\n",
    "\n",
    "un_toxic_df = pd.DataFrame.from_dict(un_toxic_df, orient = 'index', columns = ['text', 'count']).sort_values(by = ['count'], ascending = False)\n",
    "un_toxic_df = un_toxic_df.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d806d6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_df = common_df.sort_values(by = ['tcount', 'ntcount'], ascending = False)\n",
    "print(common_df[0:10])\n",
    "print(\"\\nshared comments count: \", len(common_df))\n",
    "print(\"toxic comments total: \", len(toxic_df))\n",
    "print(\"less toxic comments total: \", len(not_toxic_df))\n",
    "print(\"% of overlap for toxic: \", (round(len(common_df)/len(toxic_df),3)*100))\n",
    "print(\"% of overlap for less toxic: \", (round(len(common_df)/len(not_toxic_df),3)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636f01b5",
   "metadata": {},
   "source": [
    "This is interesting... There is a huge overlap between our more toxic and less toxic comments. Furthermore, the percentage of overlap is similar in both cases, with 76.7% overlap for toxic and 77.7% for less toxic which makese sense given that both datasets are similar sizes. This also tells me that close to 25% of each dataset is uniquely toxic and not toxic, which is also worth exploring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d16902",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Uniquely Toxic Comments, length: \", len(un_toxic_df))\n",
    "un_toxic_df[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe9b37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Uniquely Not Toxic Comments, length: \", len(un_not_toxic_df))\n",
    "un_not_toxic_df[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3108d33",
   "metadata": {},
   "source": [
    "I believe looking at this new data supports what I was saying previously, which is that all caps comments are a great way to identify toxic comments. However, as I said before, this will only help to discern the most obvious cases. This remains true with less toxic comments and proper grammar. Later, as I build my model, I should keep in mind ways to check for proper spelling and grammar. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596a5f73",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
